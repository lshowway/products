#!/usr/bin/env python3
"""
é«˜çº§æœºå™¨å­¦ä¹ é¢„æµ‹æ¨¡å‹
åŸºäºçœŸå®ICLRæ•°æ®è®­ç»ƒçš„æ¥å—ç‡é¢„æµ‹æ¨¡å‹
"""

import json
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
import joblib
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

class PaperAcceptancePredictor:
    """è®ºæ–‡æ¥å—ç‡é¢„æµ‹å™¨"""
    
    def __init__(self, models_dir="models"):
        self.models_dir = models_dir
        os.makedirs(models_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.models = {
            'random_forest': RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42
            ),
            'gradient_boosting': GradientBoostingClassifier(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42
            ),
            'logistic_regression': LogisticRegression(
                random_state=42,
                max_iter=1000
            )
        }
        
        self.scaler = StandardScaler()
        self.feature_names = []
        self.trained_models = {}
        self.ensemble_weights = {}
        
    def extract_features(self, papers_data):
        """
        ä»è®ºæ–‡æ•°æ®ä¸­æå–ç‰¹å¾
        
        Args:
            papers_data: è®ºæ–‡æ•°æ®åˆ—è¡¨
            
        Returns:
            DataFrame: ç‰¹å¾çŸ©é˜µ
            Series: æ ‡ç­¾å‘é‡ (1=æ¥å—, 0=æ‹’ç»)
        """
        
        features_list = []
        labels_list = []
        
        for paper in papers_data:
            try:
                # æå–è¯„å®¡åˆ†æ•°
                scores = []
                confidences = []
                
                for review in paper.get('reviews', []):
                    rating = review.get('rating', '-1')
                    confidence = review.get('confidence', '-1')
                    
                    if rating != '-1':
                        try:
                            score = float(rating)
                            if 1 <= score <= 10:
                                scores.append(score)
                        except:
                            pass
                    
                    if confidence != '-1':
                        try:
                            conf = float(confidence)
                            if 1 <= conf <= 5:
                                confidences.append(conf)
                        except:
                            pass
                
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆè¯„åˆ†ï¼Œè·³è¿‡
                if not scores:
                    continue
                
                # è®¡ç®—åŸºç¡€ç‰¹å¾
                features = {}
                
                # è¯„åˆ†ç»Ÿè®¡ç‰¹å¾
                features['avg_score'] = np.mean(scores)
                features['min_score'] = np.min(scores)
                features['max_score'] = np.max(scores)
                features['std_score'] = np.std(scores) if len(scores) > 1 else 0
                features['median_score'] = np.median(scores)
                features['score_range'] = np.max(scores) - np.min(scores)
                
                # è¯„åˆ†åˆ†å¸ƒç‰¹å¾
                features['num_reviews'] = len(scores)
                features['high_scores'] = sum(1 for s in scores if s >= 7)  # é«˜åˆ†æ•°é‡
                features['low_scores'] = sum(1 for s in scores if s <= 4)   # ä½åˆ†æ•°é‡
                features['mid_scores'] = sum(1 for s in scores if 4 < s < 7) # ä¸­ç­‰åˆ†æ•°é‡
                
                # è¯„åˆ†æ¯”ä¾‹ç‰¹å¾
                features['high_score_ratio'] = features['high_scores'] / len(scores)
                features['low_score_ratio'] = features['low_scores'] / len(scores)
                features['mid_score_ratio'] = features['mid_scores'] / len(scores)
                
                # è‡ªä¿¡å¿ƒç‰¹å¾
                if confidences:
                    features['avg_confidence'] = np.mean(confidences)
                    features['min_confidence'] = np.min(confidences)
                    features['max_confidence'] = np.max(confidences)
                    features['std_confidence'] = np.std(confidences) if len(confidences) > 1 else 0
                else:
                    features['avg_confidence'] = 3.0  # é»˜è®¤ä¸­ç­‰è‡ªä¿¡å¿ƒ
                    features['min_confidence'] = 3.0
                    features['max_confidence'] = 3.0
                    features['std_confidence'] = 0.0
                
                # é«˜çº§ç‰¹å¾
                features['score_confidence_corr'] = np.corrcoef(scores[:len(confidences)], confidences)[0,1] if len(confidences) > 1 else 0
                features['weighted_score'] = sum(s * c for s, c in zip(scores[:len(confidences)], confidences)) / sum(confidences) if confidences else features['avg_score']
                
                # è®ºæ–‡è´¨é‡æŒ‡æ ‡
                features['consistency_score'] = 1.0 / (1.0 + features['std_score'])  # è¯„åˆ†ä¸€è‡´æ€§
                features['controversial_score'] = features['score_range'] / 10.0      # äº‰è®®æ€§
                
                # å†³ç­–è¾¹ç•Œç‰¹å¾
                features['above_threshold_6'] = 1 if features['avg_score'] >= 6 else 0
                features['above_threshold_7'] = 1 if features['avg_score'] >= 7 else 0
                features['no_reject_score'] = 1 if features['min_score'] >= 5 else 0
                
                # æå–æ ‡ç­¾
                decision = paper.get('paper_decision', '')
                is_accepted = 1 if 'accept' in decision.lower() else 0
                
                features_list.append(features)
                labels_list.append(is_accepted)
                
            except Exception as e:
                print(f"âš ï¸  å¤„ç†è®ºæ–‡ç‰¹å¾æ—¶å‡ºé”™: {e}")
                continue
        
        # è½¬æ¢ä¸ºDataFrame
        if not features_list:
            raise ValueError("æ²¡æœ‰æå–åˆ°æœ‰æ•ˆç‰¹å¾")
        
        features_df = pd.DataFrame(features_list)
        labels_series = pd.Series(labels_list)
        
        # ä¿å­˜ç‰¹å¾åç§°
        self.feature_names = list(features_df.columns)
        
        print(f"âœ… ç‰¹å¾æå–å®Œæˆ: {len(features_df)} ä¸ªæ ·æœ¬, {len(self.feature_names)} ä¸ªç‰¹å¾")
        print(f"ğŸ“Š æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹: {labels_series.mean():.2%} æ¥å—ç‡")
        
        return features_df, labels_series
    
    def train_models(self, features_df, labels_series, test_size=0.2):
        """
        è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        
        Args:
            features_df: ç‰¹å¾çŸ©é˜µ
            labels_series: æ ‡ç­¾å‘é‡
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            dict: æ¨¡å‹æ€§èƒ½æŠ¥å‘Š
        """
        
        print("ğŸ¯ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            features_df, labels_series, 
            test_size=test_size, 
            random_state=42, 
            stratify=labels_series
        )
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # è®­ç»ƒå•ä¸ªæ¨¡å‹
        performance_report = {}
        
        for model_name, model in self.models.items():
            print(f"ğŸ”„ è®­ç»ƒ {model_name}...")
            
            # è®­ç»ƒæ¨¡å‹
            if model_name == 'logistic_regression':
                model.fit(X_train_scaled, y_train)
                y_pred = model.predict(X_test_scaled)
                y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
            else:
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                y_pred_proba = model.predict_proba(X_test)[:, 1]
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            performance = {
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred),
                'recall': recall_score(y_test, y_pred),
                'f1': f1_score(y_test, y_pred),
                'auc': roc_auc_score(y_test, y_pred_proba)
            }
            
            # äº¤å‰éªŒè¯
            if model_name == 'logistic_regression':
                cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)
            else:
                cv_scores = cross_val_score(model, X_train, y_train, cv=5)
            
            performance['cv_mean'] = cv_scores.mean()
            performance['cv_std'] = cv_scores.std()
            
            performance_report[model_name] = performance
            self.trained_models[model_name] = model
            
            print(f"   âœ… {model_name}: å‡†ç¡®ç‡={performance['accuracy']:.3f}, AUC={performance['auc']:.3f}")
        
        # è®¡ç®—é›†æˆæƒé‡ï¼ˆåŸºäºAUCæ€§èƒ½ï¼‰
        total_auc = sum(perf['auc'] for perf in performance_report.values())
        self.ensemble_weights = {
            name: perf['auc'] / total_auc 
            for name, perf in performance_report.items()
        }
        
        print(f"ğŸ“Š é›†æˆæƒé‡: {self.ensemble_weights}")
        
        # ä¿å­˜æ¨¡å‹
        self.save_models()
        
        return performance_report
    
    def predict_single(self, scores, confidences=None):
        """
        é¢„æµ‹å•ç¯‡è®ºæ–‡çš„æ¥å—æ¦‚ç‡
        
        Args:
            scores: è¯„åˆ†åˆ—è¡¨
            confidences: è‡ªä¿¡å¿ƒåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            dict: é¢„æµ‹ç»“æœ
        """
        
        if not self.trained_models:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ train_models()")
        
        # æ„é€ ç‰¹å¾
        if not confidences:
            confidences = [3.0] * len(scores)  # é»˜è®¤ä¸­ç­‰è‡ªä¿¡å¿ƒ
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(scores), len(confidences))
        scores = scores[:min_len]
        confidences = confidences[:min_len]
        
        features = {}
        
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        features['avg_score'] = np.mean(scores)
        features['min_score'] = np.min(scores)
        features['max_score'] = np.max(scores)
        features['std_score'] = np.std(scores) if len(scores) > 1 else 0
        features['median_score'] = np.median(scores)
        features['score_range'] = np.max(scores) - np.min(scores)
        
        # è¯„åˆ†åˆ†å¸ƒç‰¹å¾
        features['num_reviews'] = len(scores)
        features['high_scores'] = sum(1 for s in scores if s >= 7)
        features['low_scores'] = sum(1 for s in scores if s <= 4)
        features['mid_scores'] = sum(1 for s in scores if 4 < s < 7)
        
        features['high_score_ratio'] = features['high_scores'] / len(scores)
        features['low_score_ratio'] = features['low_scores'] / len(scores)
        features['mid_score_ratio'] = features['mid_scores'] / len(scores)
        
        # è‡ªä¿¡å¿ƒç‰¹å¾
        features['avg_confidence'] = np.mean(confidences)
        features['min_confidence'] = np.min(confidences)
        features['max_confidence'] = np.max(confidences)
        features['std_confidence'] = np.std(confidences) if len(confidences) > 1 else 0
        
        # é«˜çº§ç‰¹å¾
        features['score_confidence_corr'] = np.corrcoef(scores, confidences)[0,1] if len(scores) > 1 else 0
        features['weighted_score'] = sum(s * c for s, c in zip(scores, confidences)) / sum(confidences)
        features['consistency_score'] = 1.0 / (1.0 + features['std_score'])
        features['controversial_score'] = features['score_range'] / 10.0
        features['above_threshold_6'] = 1 if features['avg_score'] >= 6 else 0
        features['above_threshold_7'] = 1 if features['avg_score'] >= 7 else 0
        features['no_reject_score'] = 1 if features['min_score'] >= 5 else 0
        
        # è½¬æ¢ä¸ºDataFrame
        feature_vector = pd.DataFrame([features])[self.feature_names]
        
        # è·å–å„æ¨¡å‹é¢„æµ‹
        predictions = {}
        
        for model_name, model in self.trained_models.items():
            if model_name == 'logistic_regression':
                feature_scaled = self.scaler.transform(feature_vector)
                prob = model.predict_proba(feature_scaled)[0, 1]
            else:
                prob = model.predict_proba(feature_vector)[0, 1]
            
            predictions[model_name] = prob
        
        # é›†æˆé¢„æµ‹
        ensemble_prob = sum(
            predictions[name] * weight 
            for name, weight in self.ensemble_weights.items()
        )
        
        return {
            'ensemble_probability': ensemble_prob,
            'individual_predictions': predictions,
            'features_used': features,
            'confidence_level': 'high' if features['std_score'] < 1.0 else 'medium'
        }
    
    def save_models(self):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        
        model_info = {
            'feature_names': self.feature_names,
            'ensemble_weights': self.ensemble_weights,
            'training_date': datetime.now().isoformat()
        }
        
        # ä¿å­˜æ¨¡å‹æ–‡ä»¶
        for name, model in self.trained_models.items():
            joblib.dump(model, os.path.join(self.models_dir, f"{name}.pkl"))
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        joblib.dump(self.scaler, os.path.join(self.models_dir, "scaler.pkl"))
        
        # ä¿å­˜æ¨¡å‹ä¿¡æ¯
        with open(os.path.join(self.models_dir, "model_info.json"), 'w') as f:
            json.dump(model_info, f, indent=2)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° {self.models_dir}")
    
    def load_models(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        
        try:
            # åŠ è½½æ¨¡å‹ä¿¡æ¯
            with open(os.path.join(self.models_dir, "model_info.json"), 'r') as f:
                model_info = json.load(f)
            
            self.feature_names = model_info['feature_names']
            self.ensemble_weights = model_info['ensemble_weights']
            
            # åŠ è½½æ¨¡å‹
            for name in self.ensemble_weights.keys():
                model_path = os.path.join(self.models_dir, f"{name}.pkl")
                self.trained_models[name] = joblib.load(model_path)
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨
            self.scaler = joblib.load(os.path.join(self.models_dir, "scaler.pkl"))
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (è®­ç»ƒæ—¶é—´: {model_info.get('training_date', 'Unknown')})")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False

def train_predictor_from_data(data_files):
    """
    ä»æ•°æ®æ–‡ä»¶è®­ç»ƒé¢„æµ‹å™¨
    
    Args:
        data_files: æ•°æ®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    Returns:
        PaperAcceptancePredictor: è®­ç»ƒå¥½çš„é¢„æµ‹å™¨
    """
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒè®ºæ–‡æ¥å—ç‡é¢„æµ‹æ¨¡å‹")
    print("=" * 50)
    
    # åŠ è½½æ•°æ®
    all_papers = []
    for data_file in data_files:
        if not os.path.exists(data_file):
            print(f"âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            continue
        
        print(f"ğŸ“– åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
        with open(data_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    all_papers.append(json.loads(line.strip()))
    
    if not all_papers:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
    
    print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(all_papers)} ç¯‡è®ºæ–‡")
    
    # åˆ›å»ºé¢„æµ‹å™¨
    predictor = PaperAcceptancePredictor()
    
    # æå–ç‰¹å¾
    features_df, labels_series = predictor.extract_features(all_papers)
    
    # è®­ç»ƒæ¨¡å‹
    performance_report = predictor.train_models(features_df, labels_series)
    
    # æ‰“å°æ€§èƒ½æŠ¥å‘Š
    print("\nğŸ“ˆ æ¨¡å‹æ€§èƒ½æŠ¥å‘Š:")
    print("=" * 50)
    for model_name, metrics in performance_report.items():
        print(f"{model_name}:")
        print(f"  å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
        print(f"  ç²¾ç¡®ç‡: {metrics['precision']:.3f}")
        print(f"  å¬å›ç‡: {metrics['recall']:.3f}")
        print(f"  F1åˆ†æ•°: {metrics['f1']:.3f}")
        print(f"  AUC: {metrics['auc']:.3f}")
        print(f"  äº¤å‰éªŒè¯: {metrics['cv_mean']:.3f} Â± {metrics['cv_std']:.3f}")
        print()
    
    return predictor

def main():
    """ä¸»å‡½æ•° - è®­ç»ƒæ¨¡å‹"""
    
    # æ•°æ®æ–‡ä»¶åˆ—è¡¨
    data_files = [
        "nips_history_data/ICLR_2024_formatted.jsonl",
        "nips_history_data/ICLR_2025_formatted.jsonl"
    ]
    
    try:
        # è®­ç»ƒæ¨¡å‹
        predictor = train_predictor_from_data(data_files)
        
        # æµ‹è¯•é¢„æµ‹
        print("\nğŸ§ª æµ‹è¯•é¢„æµ‹åŠŸèƒ½:")
        print("-" * 30)
        
        test_cases = [
            ([8, 7, 9, 8], [4, 3, 5, 4], "é«˜è´¨é‡è®ºæ–‡"),
            ([5, 4, 6, 5], [3, 2, 3, 3], "ä¸­ç­‰è´¨é‡è®ºæ–‡"),
            ([3, 2, 4, 3], [2, 1, 2, 2], "ä½è´¨é‡è®ºæ–‡"),
            ([8, 3, 7, 6], [5, 2, 4, 3], "äº‰è®®æ€§è®ºæ–‡")
        ]
        
        for scores, confidences, description in test_cases:
            result = predictor.predict_single(scores, confidences)
            print(f"{description}: {result['ensemble_probability']:.1%} æ¥å—æ¦‚ç‡")
        
        print("\nğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥åœ¨ main.py ä¸­ä½¿ç”¨è¿™ä¸ªé«˜çº§é¢„æµ‹æ¨¡å‹äº†")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")

if __name__ == "__main__":
    main()